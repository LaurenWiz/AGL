---
title: "TensorFlow Model Dev"
author: "Sam Dunn"
date: "August 22, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tfestimators)
library(tidyverse)
library(ggplot2)
library(readxl)
library(breakDown)
library(broom)
library(Hmisc)
library(lubridate)
```

This document details the adaption of a tensor flow model to a snippet fo beach litter data.  TensorFlow is an advanced Machine Learning Program that cna be used in R or Python instances.  I chose it here because it is the current standard and has some powerful tools for future implementation of a family of models.

## Code This Demo is Based On [https://www.edgarsdatalab.com/2017/10/22/intro-to-tensorflow-in-r/]

library(tfestimators)
library(tidyverse)
library(titanic)

cols <- feature_columns(
  column_categorical_with_vocabulary_list("Sex", vocabulary_list = list("male", "female")),
  column_categorical_with_vocabulary_list("Embarked", vocabulary_list = list("S", "C", "Q", "")),
  column_numeric("Pclass")
)
model <- linear_classifier(feature_columns = cols)

titanic_set <- titanic_train %>%
  filter(!is.na(Age))

glimpse(titanic_set)
indices <- sample(1:nrow(titanic_set), size = 0.80 * nrow(titanic_set))
train <- titanic_set[indices, ]
test  <- titanic_set[-indices, ]
titanic_input_fn <- function(data) {
  input_fn(data, 
           features = c("Sex",
                        "Pclass",
                        "Embarked"), 
           response = "Survived")
}

train(model, titanic_input_fn(train))
model_eval <- evaluate(model, titanic_input_fn(test))
model_eval %>%
  flatten() %>%
  as_tibble() %>%
  glimpse()

tensorboard(model$estimator$model_dir, launch_browser = TRUE)

model_predict <- predict(model, titanic_input_fn(test))
tidy_model <- model_predict %>%
  map(~ .x %>%
        map(~.x[[1]]) %>%
        flatten() %>% 
        as_tibble()) %>%
  bind_rows() %>%
  bind_cols(test)

tidy_model

```{r  Data Import and Cleaning}
bl_df<-readRDS("tidy_debris.Rdata")

bl_df$num<-as.numeric(bl_df$num)

bl_df_w<-bl_df %>% 
  spread(type,num,fill=0,convert=T)

tofix<-colnames(bl_df_w)
#remove parentheses and other inappropriate column names
tofix<-tofix %>% 
  gsub("[()]","",.) %>% 
  gsub("[[:space:]]","_",.) %>% 
  gsub("[,]","_",.) %>% 
  gsub("[&]","",.) %>% 
  gsub("[.]","",.)
colnames(bl_df_w)<-tofix

bl_df_w$WaterbodyName<-as.factor(bl_df_w$WaterbodyName)

```

Okay, so in an ideal wolrd (and on another day) I will write a script to automatically change all column names to tidy equivalents.  I am debating whether I need to apply gather() to the different debris columns.  I did rename the factor columns because those will be needed for any filtering down the line. 

```{r}
corMatrix<-rcorr(as.matrix(bl_df_w[25:92]))
#sig_cor<-tidy(corMatrix$r)

# ++++++++++++++++++++++++++++
# flattenCorrMatrix
# ++++++++++++++++++++++++++++
# cormat : matrix of the correlation coefficients
# pmat : matrix of the correlation p-values
flattenCorrMatrix <- function(cormat, pmat) {
  ut <- upper.tri(cormat)
  data.frame(
    row = rownames(cormat)[row(cormat)[ut]],
    column = rownames(cormat)[col(cormat)[ut]],
    cor  =(cormat)[ut],
    p = pmat[ut]
    )
}

cor.df<-flattenCorrMatrix(corMatrix$r,corMatrix$P)

cig.list<-cor.df %>% 
  arrange(desc(abs(cor))) %>% #order flattened matrix by correlation value
  filter(column=="Cigarettes/cigarette_filters") %>% #select metrics of interest
  select(row) #pull items highly correlated with ciggarette abundance across all beaches
  



```


Okay, model time!  The first we'll do is identify the *feature* columns that the model will be based on.  These are the predictors we will hope best explain our response variable.  To start with I have chosen year, a factor, total volunteer hours, and the number of volunteers.
```{r Defining Feature Columns}

#ml_beach_dat$Year<-as.numeric(ml_beach_dat$Year)
#ml_beach_dat$Month<-as.numeric(ml_beach_dat$Month)
#ml_beach_dat$Day<-as.numeric(ml_beach_dat$Day)
#TF requires factors
bl_df_w$TrashWeight<-as.numeric(gsub("Lbs","",as.character(bl_df_w$TrashWeight)))
 #we need to have an idea of what columns are likely going to be important BEFORE we design the model
#replace NA values in data with 0

bl_df_w$doy<-yday(bl_df_w$EventDate)

cols <- feature_columns(
  column_numeric("doy"),
  column_numeric("ActualCleanupHours"),
  column_numeric("ActualParticipantCount"),
  column_numeric("TrashWeight"),
  column_categorical_with_identity("WaterbodyName",num_buckets = count(levels(bl_df_w$WaterbodyName))),
  column_numeric("Caps__lids"),
  column_numeric("Balloons"),
  column_numeric("Beverage_Bottles_Glass"),
  column_numeric("Beverage_Cans"),
  column_numeric("Batteries"),
  column_numeric("Beverage_Bottles_Plastic"),
  column_numeric("6-pack-holders"),
  column_numeric("Cigar_tips"),
  column_numeric("Bottle_Caps_Metal"),
  column_numeric("Bait_containers"),
  column_numeric("Car/car_parts"),
  column_numeric("Cigarette_Lighters"),
  column_numeric("Appliances_refrigerators__washers__etc"),
  column_numeric("55-gal_drums")
)

```


No we need to divide the datset into training and evaluation sets.  We will train a model on roughly half fo the data (by row) and then see how well the model can predit the outcome observed ont he other half.  This process is what taes the longest because it requires fine-tuning what the feature columns are and the meothod by which they are used to train a model.  Unliekc lassical statistics, the tensorflow software does the assembly for us!
```{r Define Train and Eval sets}
row_indices<-sample(1:nrow(bl_df_w),
                    size=0.2*nrow(bl_df_w))
beach_train<-bl_df_w[row_indices,]
beach_test<-bl_df_w[-row_indices,]


model<-linear_regressor(feature_columns = cols) #linear classifieres are used to predict continuous outcomes from categoricala nd continuous feature cols.  linear regressos handle only continuoiurs.  other predictors are needed for different outcomes

#Error in .Call(`_reticulate_py_call_impl`, x, args, keywords) : reached elapsed time limit <- this is a seg fault error code!
```


Now we will define the model for TensorFlow by defining a model function.  This is much more simple than it sounds.  Essentially we are creating a function that requires the *data* and we identify the *features* from above as well as the *response* we are trying to predict.
```{r}
cig_beach_fn<-function(data){
  input_fn(data,
           features=c("doy",
                      "ActualCleanupHours",
                      "ActualParticipantCount",
                      "TrashWeight",
                      "WaterbodyName",
                      "Caps__lids",
                      "Balloons",
                      "Beverage_Bottles_Glass",
                      "Beverage_Cans",
                      "Batteries",
                      "Beverage_Bottles_Plastic",
                      "6-pack-holders",
                      "Cigar_tips",
                      "Bottle_Caps_Metal",
                      "Bait_containers",
                      "Car/car_parts",
                      "Bottle_Caps_Plastic",
                      "Cigarette_Lighters",
                      "Appliances_refrigerators__washers__etc",
                      "55-gal_drums"),
           response="Cigarettes/cigarette_filters")
}
```


```{r}
train(model,cig_beach_fn(beach_train))


predictions_test<-predict(
  model,
  input_fn=cig_beach_fn(beach_test)
)
tidy_model <- predictions_test %>%
  map(~ .x %>%
        map(~.x[[1]]) %>%
        flatten() %>% 
        as_tibble()) %>%
  bind_rows() %>%
  bind_cols(test)

tidy_model 


predictions_all<-predict(
  model,
  input_fn = cig_beach_fn(bl_df_w)
)

eval_test<-evaluate(
  model,
  input_fn=cig_beach_fn(beach_test)
)

eval_test %>% 
  flatten() %>% 
  as_tibble() %>% 
  glimpse()

eval_all<-evaluate(
  model,
  input_fn = cig_beach_fn(bl_df_w)
)
eval_all %>% 
  flatten() %>% 
  as_tibble() %>% 
  glimpse()
```


